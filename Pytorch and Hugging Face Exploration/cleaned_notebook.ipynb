{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JehQ8e071rFk"
   },
   "source": [
    "# Goal: PyTorch and HuggingFace Exploration\n",
    "\n",
    "PyTorch and HuggingFace have emerged as powerful tools for developing and deploying neural networks.\n",
    "\n",
    "In this project, we will explore the capabilities of PyTorch and HuggingFace, uncovering hidden treasures on the way.\n",
    "\n",
    "We have two parts:\n",
    "* Familiarization with PyTorch\n",
    "* Familarization with HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OFmm3QEY1rFl"
   },
   "source": [
    "## Familiarization with PyTorch\n",
    "\n",
    "Basics of PyTorch, including tensors, neural net parts, loss functions, and optimizers. This provides us with a foundation for understanding and utilizing its capabilities in developing and training neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2Amq17_1rFl"
   },
   "source": [
    "### PyTorch tensors\n",
    "\n",
    "PyTorch tensors documentation [Learn More](https://pytorch.org/docs/stable/tensors.html).\n",
    "\n",
    "In the following cell, I create a tensor named `my_tensor` of size 3x3 with random values. The tensor should be created on the GPU if available. Then I print the tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4237,
     "status": "ok",
     "timestamp": 1753217737119,
     "user": {
      "displayName": "Srishti Jain",
      "userId": "17735186882349502119"
     },
     "user_tz": 240
    },
    "id": "H1gjq5881rFm",
    "outputId": "9f414ca6-6b29-4716-9adf-22b273cd33b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Tip: Using torch.cuda.is_available() to check if GPU is available\n",
    "\n",
    "import torch\n",
    "\n",
    "# Set the device to be used for the tensor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Create a tensor on the appropriate device\n",
    "my_tensor = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], device=device)\n",
    "\n",
    "# Print the tensor\n",
    "print(my_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1753217741554,
     "user": {
      "displayName": "Srishti Jain",
      "userId": "17735186882349502119"
     },
     "user_tz": 240
    },
    "id": "-N_4HlSH1rFm",
    "outputId": "318e815e-ed0b-44dd-c024-8d1ac0bfd66b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success!\n"
     ]
    }
   ],
   "source": [
    "# Check the previous cell\n",
    "\n",
    "assert my_tensor.device.type in {\"cuda\", \"cpu\"}\n",
    "assert my_tensor.shape == (3, 3)\n",
    "\n",
    "print(\"Success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aqq6YBXQ1rFm"
   },
   "source": [
    "### Neural Net Constructor Kit `torch.nn`\n",
    "\n",
    "We can think of the `torch.nn` ([documentation](https://pytorch.org/docs/stable/nn.html)) module as a constructor kit for neural networks. It provides the building blocks for creating neural networks, including layers, activation functions, loss functions, and more.\n",
    "\n",
    "Instructions:\n",
    "\n",
    "We create a three layer Multi-Layer Perceptron (MLP) neural network with the following specifications:\n",
    "\n",
    "- Input layer: 784 neurons\n",
    "- Hidden layer: 128 neurons\n",
    "- Output layer: 10 neurons\n",
    "\n",
    "Using the ReLU activation function for the hidden layer and the softmax activation function for the output layer. Then we print the neural network.\n",
    "\n",
    "Tip: MLP's use \"fully-connected\" or \"dense\" layers. In PyTorch's `nn` module, this type of layer has a different name. See the examples in [this tutorial](https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.html) to find out more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1753217744496,
     "user": {
      "displayName": "Srishti Jain",
      "userId": "17735186882349502119"
     },
     "user_tz": 240
    },
    "id": "rRl2372q1rFn",
    "outputId": "e84861ad-4e45-45aa-b39e-1567c8e15caa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyMLP(\n",
      "  (fc1): Linear(in_features=784, out_features=128, bias=True)\n",
      "  (fc2): Linear(in_features=128, out_features=10, bias=True)\n",
      "  (relu): ReLU()\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class MyMLP(nn.Module):\n",
    "    \"\"\"My Multilayer Perceptron (MLP)\n",
    "\n",
    "    Specifications:\n",
    "\n",
    "        - Input layer: 784 neurons\n",
    "        - Hidden layer: 128 neurons with ReLU activation\n",
    "        - Output layer: 10 neurons with softmax activation\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super(MyMLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass the input to the second layer\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        # Apply ReLU activation\n",
    "        x = self.relu(x)\n",
    "\n",
    "        # Pass the result to the final layer\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        # Apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "my_mlp = MyMLP()\n",
    "print(my_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1753217747546,
     "user": {
      "displayName": "Srishti Jain",
      "userId": "17735186882349502119"
     },
     "user_tz": 240
    },
    "id": "mU7Ta9db1rFn"
   },
   "outputs": [],
   "source": [
    "# Check the work here:\n",
    "\n",
    "\n",
    "# Check the number of inputs\n",
    "assert my_mlp.fc1.in_features == 784\n",
    "\n",
    "# Check the number of outputs\n",
    "assert my_mlp.fc2.out_features == 10\n",
    "\n",
    "# Check the number of nodes in the hidden layer\n",
    "assert my_mlp.fc1.out_features == 128\n",
    "\n",
    "# Check that my_mlp.fc1 is a fully connected layer\n",
    "assert isinstance(my_mlp.fc1, nn.Linear)\n",
    "\n",
    "# Check that my_mlp.fc2 is a fully connected layer\n",
    "assert isinstance(my_mlp.fc2, nn.Linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DYMC-QCl1rFn"
   },
   "source": [
    "### PyTorch Loss Functions and Optimizers\n",
    "\n",
    "PyTorch comes with a number of built-in loss functions and optimizers that can be used to train neural networks. The loss functions are implemented in the `torch.nn` ([documentation](https://pytorch.org/docs/stable/nn.html#loss-functions)) module, while the optimizers are implemented in the `torch.optim` ([documentation](https://pytorch.org/docs/stable/optim.html)) module.\n",
    "\n",
    "\n",
    "Instructions:\n",
    "\n",
    "- We create a loss function using the `torch.nn.CrossEntropyLoss` ([documentation](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html#torch.nn.CrossEntropyLoss)) class.\n",
    "- Then we create an optimizer using the `torch.optim.SGD` ([documentation](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD)) class with a learning rate of 0.01.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 1332,
     "status": "ok",
     "timestamp": 1753217752482,
     "user": {
      "displayName": "Srishti Jain",
      "userId": "17735186882349502119"
     },
     "user_tz": 240
    },
    "id": "TKqLFtM41rFn"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Loss function\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer (by convention we use the variable optimizer)\n",
    "optimizer = torch.optim.SGD(my_mlp.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1753217754022,
     "user": {
      "displayName": "Srishti Jain",
      "userId": "17735186882349502119"
     },
     "user_tz": 240
    },
    "id": "WmObRvix1rFn"
   },
   "outputs": [],
   "source": [
    "# Check\n",
    "\n",
    "assert isinstance(\n",
    "    loss_fn, nn.CrossEntropyLoss\n",
    "), \"loss_fn should be an instance of CrossEntropyLoss\"\n",
    "assert isinstance(optimizer, torch.optim.SGD), \"optimizer should be an instance of SGD\"\n",
    "assert optimizer.defaults[\"lr\"] == 0.01, \"learning rate should be 0.01\"\n",
    "assert optimizer.param_groups[0][\"params\"] == list(\n",
    "    my_mlp.parameters()\n",
    "), \"optimizer should be passed the MLP parameters\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRjyuJFN1rFn"
   },
   "source": [
    "### PyTorch Training Loops\n",
    "\n",
    "PyTorch makes writing a training loop easy!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 109,
     "status": "ok",
     "timestamp": 1753217756051,
     "user": {
      "displayName": "Srishti Jain",
      "userId": "17735186882349502119"
     },
     "user_tz": 240
    },
    "id": "YqOPgqf51rFn",
    "outputId": "b97e1609-3c8a-41f9-b5d4-d6c4dc5eeee0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, batch 0: 2.30079\n",
      "Epoch 0, batch 10: 2.30109\n",
      "Epoch 0, batch 20: 2.30104\n",
      "Epoch 1, batch 0: 2.30516\n",
      "Epoch 1, batch 10: 2.30392\n",
      "Epoch 1, batch 20: 2.30504\n",
      "Epoch 2, batch 0: 2.30441\n",
      "Epoch 2, batch 10: 2.30684\n",
      "Epoch 2, batch 20: 2.30452\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "def fake_training_loaders():\n",
    "    for _ in range(30):\n",
    "        yield torch.randn(64, 784), torch.randint(0, 10, (64,))\n",
    "\n",
    "\n",
    "for epoch in range(3):\n",
    "    # Create a training loop\n",
    "    for i, data in enumerate(fake_training_loaders()):\n",
    "        # Every data instance is an input + label pair\n",
    "        x, y = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass (predictions)\n",
    "        y_pred = my_mlp(x)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(y_pred, y)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch {epoch}, batch {i}: {loss.item():.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1753217758372,
     "user": {
      "displayName": "Srishti Jain",
      "userId": "17735186882349502119"
     },
     "user_tz": 240
    },
    "id": "yVcQT7tt1rFo"
   },
   "outputs": [],
   "source": [
    "# Check\n",
    "\n",
    "assert abs(loss.item() - 2.3) < 0.1, \"the loss should be around 2.3 with random data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zPRr4QYn1rFo"
   },
   "source": [
    "## Getting to know HuggingFace\n",
    "\n",
    "HuggingFace is a popular destination for pre-trained models and datasets that can be applied to a variety of tasks quickly and easily. In this section, we will explore the capabilities of HuggingFace and learn how to use it to build and train neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0xHN87DQ1rFo"
   },
   "source": [
    "## Download a model from HuggingFace and use it for sentiment analysis\n",
    "\n",
    "HuggingFace provides a number of pre-trained models that can be used for a variety of tasks. Here, we will use the `distilbert-base-uncased-finetuned-sst-2-english` model to perform sentiment analysis on a movie review.\n",
    "\n",
    "Instructions:\n",
    "- Review the [AutoModel tutorial](https://huggingface.co/docs/transformers/quicktour#automodel) on the HuggingFace website.\n",
    "- Instantiate an AutoModelForSequenceClassification model using the `distilbert-base-uncased-finetuned-sst-2-english` model.\n",
    "- Instantiate an AutoTokenizer using the `distilbert-base-uncased-finetuned-sst-2-english` model.\n",
    "- Define a function that will get a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 10810,
     "status": "ok",
     "timestamp": 1753217772020,
     "user": {
      "displayName": "Srishti Jain",
      "userId": "17735186882349502119"
     },
     "user_tz": 240
    },
    "id": "Mw4LLTz61rFo"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "pt_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    ")\n",
    "\n",
    "\n",
    "def get_prediction(review):\n",
    "    \"\"\"Given a review, return the predicted sentiment\"\"\"\n",
    "\n",
    "    # Tokenize the review\n",
    "    # (Get the response as tensors and not as a list)\n",
    "    inputs = tokenizer(review, return_tensors=\"pt\")\n",
    "\n",
    "    # Perform the prediction (get the logits)\n",
    "    outputs = pt_model(**inputs)\n",
    "\n",
    "    # Get the predicted class (corresponding to the highest logit)\n",
    "    predictions = torch.argmax(outputs.logits, dim=-1)\n",
    "\n",
    "    return \"positive\" if predictions.item() == 1 else \"negative\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1753217776422,
     "user": {
      "displayName": "Srishti Jain",
      "userId": "17735186882349502119"
     },
     "user_tz": 240
    },
    "id": "qB7EcEzH1rFo",
    "outputId": "1ea4f963-d5df-42be-8a9f-e2e9b792179f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: This movie is not so great :(\n",
      "Sentiment: negative\n",
      "Review: This movie rocks!\n",
      "Sentiment: positive\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "\n",
    "review = \"This movie is not so great :(\"\n",
    "\n",
    "print(f\"Review: {review}\")\n",
    "print(f\"Sentiment: {get_prediction(review)}\")\n",
    "\n",
    "assert get_prediction(review) == \"negative\", \"The prediction should be negative\"\n",
    "\n",
    "\n",
    "review = \"This movie rocks!\"\n",
    "\n",
    "print(f\"Review: {review}\")\n",
    "print(f\"Sentiment: {get_prediction(review)}\")\n",
    "\n",
    "assert get_prediction(review) == \"positive\", \"The prediction should be positive\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rS3dhVVA1rFo"
   },
   "source": [
    "### Download a dataset from HuggingFace\n",
    "\n",
    "HuggingFace provides a number of datasets that can be used for a variety of tasks. Here, we will use the `imdb` dataset and pass it to the model we instantiated in the previous exercise.\n",
    "\n",
    "Instructions:\n",
    "- Review the [loading a dataset](https://huggingface.co/docs/datasets/v1.11.0/loading_datasets.html) documentation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 10007,
     "status": "ok",
     "timestamp": 1753217656349,
     "user": {
      "displayName": "Srishti Jain",
      "userId": "17735186882349502119"
     },
     "user_tz": 240
    },
    "id": "LzF8fe5I6jQX",
    "outputId": "51b30ea3-c079-453b-bd86-f318829dcbff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.0.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2025.7.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
      "Collecting fsspec\n",
      "  Downloading fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.33.4)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.14.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.7.14)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-4.0.0-py3-none-any.whl (494 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m494.8/494.8 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: fsspec, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.7.0\n",
      "    Uninstalling fsspec-2025.7.0:\n",
      "      Successfully uninstalled fsspec-2025.7.0\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 2.14.4\n",
      "    Uninstalling datasets-2.14.4:\n",
      "      Successfully uninstalled datasets-2.14.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2025.7.0 requires fsspec==2025.7.0, but you have fsspec 2025.3.0 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\n",
      "torch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed datasets-4.0.0 fsspec-2025.3.0\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "id": "2fd8d71edd074924a6e4816bc4a41f5b",
       "pip_warning": {
        "packages": [
         "datasets",
         "fsspec"
        ]
       }
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install --upgrade datasets fsspec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368,
     "referenced_widgets": [
      "74ae81a7ab264095abc731b109ee7e3b",
      "7f75fc603b3548b390316002b83e3e90",
      "c27e5152faeb4b90999ce9ad7d411d38",
      "2c347ec48b444ccbbf75d2a16265f6b7",
      "f0ddd885af8c4247978ab916b07c36d8",
      "de310eb1d8254933b6ab73bbe3b2d084",
      "913906913d3e43b0946b2d0f6ef897e5",
      "550ebbc5653a46f18571f42c6d01b965",
      "ca855da3f9fa4a988c898790bb5551f4",
      "bf8f1d130f3f434887ba7affa4608665",
      "604b492f9e55463f9077da0e1a3a233c",
      "a0c3f5c7564445bd88cb2193c2b23cd4",
      "cdaf10393ada43b3b4359602a2a4c8cb",
      "bcbacf57158348a7b8a6c50c95cd3a6b",
      "888b9c2ce55c4352960558aaa7d0af6c",
      "3f58e9c574fc4552bc27770474e476bc",
      "ce0e6cd6d03b4e0e843024d3ce845f1f",
      "63ee514364754019bd688a5dd50bedb4",
      "59560125e0d841bea26bd753227efc21",
      "80db16a299874dde9ebd2160feaaa231",
      "0d3e7064419b40f0bb4bcda74c7097d3",
      "34a0d5d5d45245e2b8ed99e33e9c1967",
      "ba0d382766aa496e866ff9ef65f764b7",
      "d60d0057bce64d84af935d9acfe234bd",
      "9cf9b75b98b849a283dbc64ad8ff92ad",
      "3f94b51a620548da8030743e0e740776",
      "349353bd07e34b34963100c36dd711c7",
      "0ecdeed7f7a94366a890e4504c2fadb0",
      "30b31cb06fd240f484afd4d8273d57b2",
      "8abb3307f6a54efd8922022bafbc9e19",
      "9eefcc6f4dfb4d86af30f6b6f7574f13",
      "fb9a34d7bc3e466a8d1fd6e01d1b8cca",
      "4ae0be19bb694af88defe71a82cc8bc5",
      "97774c7b582e4afab0d1a9fcbba1499a",
      "76a7a94ba4f04191867135f8d89c75a3",
      "fc15dbd4529141d580ac41615225bbc7",
      "0b2019a66f904726849146708e06c609",
      "c0fa63029c6947b792bbf7d76630f005",
      "75d6393517d74c469a619fd55ef93841",
      "517ab8ea41fc4fa08fa545d45768436e",
      "19b3100d6ab949d8be54b300179a89b9",
      "13fcbc477d904c61942343cb65d98c72",
      "1a7b69987e5a45029885ba171475fcb9",
      "672ae958150a44c2a936b43759ad7bd4",
      "f3f76eb4281b4de381879078b01da058",
      "ab28872109c345f8baa8e307e29b2488",
      "11bc732e37d245b6a5627d8380bd41f8",
      "37984154d83a480f96411406f9f97dad",
      "93b18e38deca4d3297cd4043a40db0c8",
      "e5582ad1597146569f777db2bba35fe5",
      "a73b013e900e4006870017d28984c22a",
      "2045643d112d49f89d0c081f2f0ca9d8",
      "3f091d7128624d4685b90c4d1b16cfa6",
      "333ad49c147d4d9a8452d5745f3f0146",
      "7b51a16daafb443e9b66b119412f5e73",
      "1eecc1d113c449be98866f7de629f959",
      "2395471dab324a7c90f80ae6a41e3792",
      "e610f30383a242bd910739fc706e8853",
      "ec83d28ad6764bb6817836c625c7a21a",
      "826fedc102fb44f18f0b1d5bd3b8bae6",
      "569bca05d0f04c869416554ac6239ea9",
      "25d9d30d557e409eba26b5a29871a1c5",
      "c21b2f6aefc3437e9b157f265451c00e",
      "9c2e8dfa91fe45ac913ab76c073beaa0",
      "2c33d3777e404873b10f22df3a058bc2",
      "e5e98af49aa148a1bb164071b4b9155d",
      "ab36f7303c4240368bfc123732600fe3",
      "10418cb664ac4094894f088d4f3210c2",
      "fa991ebe2e7545829e9d38c79b7ec2cb",
      "f06d9d1c1c584f3d950bffbbde18214a",
      "0f7705c15a024c4b9ca0c2fd108c9d14",
      "1986d175932549a1b2564e72ffce511f",
      "78ae600624b84d0ca7b1c57a3c7c6545",
      "fa00af79617e44e8bc69a9739ea7e161",
      "5cc255f587e24dfd89026dde89643fc2",
      "3b987d485f4d4b64a7600ed650850e40",
      "2aa39565eb254482890b81372affbd6f"
     ]
    },
    "executionInfo": {
     "elapsed": 17467,
     "status": "ok",
     "timestamp": 1753217685349,
     "user": {
      "displayName": "Srishti Jain",
      "userId": "17735186882349502119"
     },
     "user_tz": 240
    },
    "id": "kpvjoxk-1rFo",
    "outputId": "c7a435b9-81a2-4088-e600-5075fd919194"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74ae81a7ab264095abc731b109ee7e3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0c3f5c7564445bd88cb2193c2b23cd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train-00000-of-00001.parquet:   0%|          | 0.00/21.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0d382766aa496e866ff9ef65f764b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "test-00000-of-00001.parquet:   0%|          | 0.00/20.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97774c7b582e4afab0d1a9fcbba1499a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "unsupervised-00000-of-00001.parquet:   0%|          | 0.00/42.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3f76eb4281b4de381879078b01da058",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1eecc1d113c449be98866f7de629f959",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab36f7303c4240368bfc123732600fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating unsupervised split:   0%|          | 0/50000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"imdb\", split=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1753217784408,
     "user": {
      "displayName": "Srishti Jain",
      "userId": "17735186882349502119"
     },
     "user_tz": 240
    },
    "id": "LVdj-lRk1rFo",
    "outputId": "22838e2a-20aa-4788-d879-7336ce78578f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'label': 0,\n",
      " 'text': 'I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV '\n",
      "         'are usually underfunded, under-appreciated and misunderstood. I '\n",
      "         'tried to like this, I really did, but it is to good TV sci-fi as '\n",
      "         'Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap '\n",
      "         \"cardboard sets, stilted dialogues, CG that doesn't match the \"\n",
      "         'background, and painfully one-dimensional characters cannot be '\n",
      "         \"overcome with a 'sci-fi' setting. (I'm sure there are those of you \"\n",
      "         \"out there who think Babylon 5 is good sci-fi TV. It's not. It's \"\n",
      "         'clichéd and uninspiring.) While US viewers might like emotion and '\n",
      "         'character development, sci-fi is a genre that does not take itself '\n",
      "         'seriously (cf. Star Trek). It may treat important issues, yet not as '\n",
      "         \"a serious philosophy. It's really difficult to care about the \"\n",
      "         'characters here as they are not simply foolish, just missing a spark '\n",
      "         'of life. Their actions and reactions are wooden and predictable, '\n",
      "         \"often painful to watch. The makers of Earth KNOW it's rubbish as \"\n",
      "         'they have to always say \"Gene Roddenberry\\'s Earth...\" otherwise '\n",
      "         \"people would not continue watching. Roddenberry's ashes must be \"\n",
      "         'turning in their orbit as this dull, cheap, poorly edited (watching '\n",
      "         'it without advert breaks really brings this home) trudging Trabant '\n",
      "         'of a show lumbers into space. Spoiler. So, kill off a main '\n",
      "         'character. And then bring him back as another actor. Jeeez! Dallas '\n",
      "         'all over again.'}\n"
     ]
    }
   ],
   "source": [
    "# Check\n",
    "\n",
    "from pprint import pprint\n",
    "\n",
    "from datasets import Dataset\n",
    "\n",
    "assert isinstance(dataset, Dataset), \"The dataset should be a Dataset object\"\n",
    "assert set(dataset.features.keys()) == {\n",
    "    \"label\",\n",
    "    \"text\",\n",
    "}, \"The dataset should have a label and a text feature\"\n",
    "\n",
    "# Show the first example\n",
    "pprint(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "367crVn21rFo"
   },
   "source": [
    "### Now let's use the pre-trained model!\n",
    "\n",
    "Let's make some predictions.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2306,
     "status": "ok",
     "timestamp": 1753217796056,
     "user": {
      "displayName": "Srishti Jain",
      "userId": "17735186882349502119"
     },
     "user_tz": 240
    },
    "id": "vUJtYz331rFp",
    "outputId": "187d76f6-c5b7-423f-c326-ef747b97ec18"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review: I got Monster Man in a box set of three films where I mainly wanted the other tw \n",
      "... ous, often gnarly splatter comedy that should endear itself to fans of the same.\n",
      "Label: positive\n",
      "Prediction: positive\n",
      "\n",
      "Review: Five minutes in, i started to feel how naff this was looking, you've got a compl \n",
      "... for anyone who likes their horror with several side orders of gore and attitude.\n",
      "Label: positive\n",
      "Prediction: positive\n",
      "\n",
      "Review: I caught this movie on the Sci-Fi channel recently. It actually turned out to be \n",
      "... e more than passable for the horror/slasher buff. Definitely worth checking out.\n",
      "Label: positive\n",
      "Prediction: positive\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Get the last 3 reviews\n",
    "reviews = dataset[\"text\"][-3:]\n",
    "\n",
    "# Get the last 3 labels\n",
    "labels = dataset[\"label\"][-3:]\n",
    "\n",
    "# Check\n",
    "for review, label in zip(reviews, labels):\n",
    "    # Let's use your get_prediction function to get the sentiment\n",
    "    # of the review!\n",
    "    prediction = get_prediction(review)\n",
    "\n",
    "    print(f\"Review: {review[:80]} \\n... {review[-80:]}\")\n",
    "    print(f'Label: {\"positive\" if label else \"negative\"}')\n",
    "    print(f\"Prediction: {prediction}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oydF25Ga8N6k"
   },
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "path = \"cleaned_notebook.ipynb\"\n",
    "\n",
    "# Get the current notebook contents\n",
    "from google.colab import _message\n",
    "nb = _message.blocking_request('get_ipynb')['ipynb']\n",
    "\n",
    "# Remove 'widgets' metadata if present\n",
    "if 'widgets' in nb['metadata']:\n",
    "    del nb['metadata']['widgets']\n",
    "\n",
    "# Save the cleaned notebook\n",
    "with open(path, \"w\") as f:\n",
    "    nbformat.write(nbformat.from_dict(nb), f)\n",
    "\n",
    "print(f\"Cleaned notebook saved as {path}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
