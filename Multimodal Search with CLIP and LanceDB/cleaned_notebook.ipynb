{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8c17377",
   "metadata": {
    "id": "e8c17377"
   },
   "source": [
    "# Multimodal Search\n",
    "\n",
    "In this project, we will learn how to use vector databases to search through images using natural language.\n",
    "\n",
    "We will be searching through an open source image dataset using an open source model called CLIP.\n",
    "This model is able to encode both images and text into the same embedding space, allowing us to retrieve images that are similar to a user question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab7501c-ae21-462f-b744-c9fc3f6ad965",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30043,
     "status": "ok",
     "timestamp": 1753991858656,
     "user": {
      "displayName": "Srishti Jain",
      "userId": "17735186882349502119"
     },
     "user_tz": 240
    },
    "id": "eab7501c-ae21-462f-b744-c9fc3f6ad965",
    "outputId": "fd1d1071-c1da-4b0c-e769-0a5d3bf72c7e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.8/34.8 MB\u001b[0m \u001b[31m25.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install --quiet datasets gradio lancedb pandas transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfec2b3",
   "metadata": {
    "id": "ebfec2b3"
   },
   "source": [
    "## Setup CLIP model\n",
    "\n",
    "First, let's prepare the [CLIP](https://huggingface.co/docs/transformers/model_doc/clip) model to encode the images.\n",
    "We want to setup two things:\n",
    "1. a model to encode the image\n",
    "2. a processor to prepare the image to be encoded\n",
    "\n",
    "\n",
    "\n",
    "By reading the CLIP documentation, we see that we can use the `from_pretrained` classmethods to initialize the model and the processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d25d2d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 450,
     "referenced_widgets": [
      "22cc573904764798a8ebe195bf59cd5b",
      "5e2a6349c9114e6ca4014ebc801d339e",
      "4ee6814424804364af3cf60b0e096f95",
      "1bef245a83504075a69055c3e87e5e3b",
      "ab5f8293436641e2b39484f789b16705",
      "4cd74d5bc3c446efa7498cb30eb98a2c",
      "97111d0452734cf99f0130f7e18971fe",
      "9121c7d958954104be793f449e51e648",
      "1da2cc8f4266490c977168f5c9ca71a8",
      "fd95efbfcfb641f3979b69f391317c09",
      "5ce890b06b864e38bbccda6f46027734",
      "5efd10053eb54378b5df62aa0982d2d2",
      "c42d84144cfc4c34b10704c8bb902186",
      "629e7734df994fe99c65a072ebf77d3f",
      "682ea2dde07b459f9fae36a12b9aaabf",
      "2f911f355fde4b0aaf231ce023b83288",
      "723f144210164b0eb0cbb595ecc8cbd2",
      "ad46d0bed1cc4db7a2ba8806a072b905",
      "551bbcf204bb49908c9eb4be622d0f5d",
      "566a9cd1176d466ba446db597578d785",
      "007ce1d2647b4e1d85ba4c7ba2c9feae",
      "915a88cd816449e4b36084ff26f4119b",
      "d4ee12dc091944e2b1082bee36a02972",
      "eefa9f0c0561411b96862677b7d34132",
      "c67e5d2c085a4982a24744b556a9e749",
      "2e5cf2e8cb624fbb821852b7f2ca2f78",
      "8096358f16e24a11af1cde84086ed45f",
      "4a7a8255b3644165b58382f6ca1d8660",
      "730585ade3e641d5bae323ad41a514a6",
      "c70431b257a5420d862fe7d6f0edbf40",
      "2a315a2f762d47caa2b8596e6ea58a13",
      "65a9b751af0d4f53b6e4d50703dad0fa",
      "96cd113cbbeb481293d01fe312608b2f",
      "8849e5f42fcc479e8d470dbf4164db32",
      "010481df5cc14d4b82edfb53a95e5186",
      "9ff772c48e35484494a850bdeb2933d2",
      "852dd0e285fd4743a59c61f9f23ed729",
      "9f5eef712cfa4ff8bb2332867bbb9b2f",
      "bdcf00469f284a82ae9a1e63c46355f7",
      "acbceabe38074427b3e861d0cb80a8a9",
      "9de2bd2902a2440998a565a474ece837",
      "608d73aa37aa495d9b90fc9c68e21895",
      "fac8b7a33f8f42fe864f7669a2c31331",
      "b62ecb7abb074b7ba34c7f4d2e58514e",
      "b4128c7cc72a4b278388336cd62630a3",
      "84411dd6184e49d4b16d4c662d590703",
      "89827aabe704471ca8e2132597d1b75b",
      "2bd1e0e400dc458ca1334fb11db9a372",
      "546425c84d5447ac992d936f30e40f00",
      "9617afdd44584a8d843ac3444c4b1933",
      "7eb6e2cb45b34260b06bd030c6bf12ae",
      "1abd963f8c7a4cb7a6faa36990e4acc8",
      "51ad53032d784c4f9ed5c307ac02fb83",
      "c53ce05eed9f4adc8ff10a01f26dd2b4",
      "73245c8ce22b40dda34319305dfc2925",
      "4bc294d7edd84356a5efbc4bfce1fa46",
      "585d7fcbb6564c4c928b6d262cfd7cac",
      "e66d1c615496415692db9406206c7083",
      "b1437ed4ace446fe9c0644a3e91514ba",
      "c7864405035b49baaff8a7e9d5de54c5",
      "3416aa1657e946709991a52fd963b7b2",
      "3d681202a2e6446da3a8ca0447ac3571",
      "549fbde3dfa94a0581e5dc1ebd9ec384",
      "524bb278d39d4799a28d12495f6b301e",
      "d8a7b07929db4d7dbedc3b6c257c8ca9",
      "4835cd1e976445ee88446788453680c3",
      "78120060244647df85c173f3d44c83a8",
      "94ac6228e5ff4049bf5a89d02380c2c7",
      "d3f5b3740cb8420e97a8f19d58ef6b16",
      "3afc09ad141e4af4aa0ce1fb3fb18363",
      "5f14e1dfb8394fa996f97244a7dd2878",
      "80ffcee9c1e94c4e808e2e76a45d182c",
      "75e360ab909e4b2eb8dc1393dbcbca15",
      "3e751a7f168c4bf2ac555dbbb615694a",
      "4140d76470bc402398c46adebe0510be",
      "b8f3893d42a748cf8a04dbebdf9d6861",
      "e4f7faa418ac493ab6de0c86b5285ac7",
      "2c777e03ed9844a08ae5f0e195689a5c",
      "5c0ae4326bbc49039f731ef5a7cb24a7",
      "ac3c0bbaadf840bb99f6aea779679bd2",
      "058b95e5f9bc4c949068c0a0d74e1542",
      "04fa2d880f3347f887531ad4fd28e382",
      "7aa387ba0f014d3b85e45aa7e261563c",
      "90760506b73c4d9f81b27689a9dde886",
      "a68dc9bdd0ee4a3fa22c023392069dd7",
      "d64f5d282d724cdbbad797b87d0c05ba",
      "92f42e5f74bd411a802ebdae63f273b8",
      "5d17c726b2d04b8fb4e76e062fbf007c",
      "fc02cb56a5574bfcb8f35f37156a41a0",
      "73517171a7b24e5182786277414a4990",
      "d079c7ecdb2b4e83825c9b12c6895de9",
      "cb3934bcad7548ab8db90cc9b81c2a64",
      "8cadc3280c354da389648abf946eb860",
      "ad2cc87bb04a442bb1f0f8b4a8d6242d",
      "1e89bb015c3a4de29cc235276f6a51c8",
      "e329d537463246a082ef9cb040dc356e",
      "97137129d9a6481da44209739d9b1053",
      "7a2d18bab91b4326a21de1bbbab43b5d",
      "a232b64947cf47b49ccf92d9f9873847"
     ]
    },
    "executionInfo": {
     "elapsed": 57011,
     "status": "ok",
     "timestamp": 1753991974273,
     "user": {
      "displayName": "Srishti Jain",
      "userId": "17735186882349502119"
     },
     "user_tz": 240
    },
    "id": "f3d25d2d",
    "outputId": "a05c0f7f-2cda-4ddd-c177-00f642503fce"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
      "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
      "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
      "You will be able to reuse this secret in all of your notebooks.\n",
      "Please note that authentication is recommended but still optional to access public models or datasets.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22cc573904764798a8ebe195bf59cd5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5efd10053eb54378b5df62aa0982d2d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4ee12dc091944e2b1082bee36a02972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/605M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8849e5f42fcc479e8d470dbf4164db32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/316 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4128c7cc72a4b278388336cd62630a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/592 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc294d7edd84356a5efbc4bfce1fa46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "78120060244647df85c173f3d44c83a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c777e03ed9844a08ae5f0e195689a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc02cb56a5574bfcb8f35f37156a41a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/389 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import CLIPModel, CLIPProcessor\n",
    "\n",
    "MODEL_ID = \"openai/clip-vit-base-patch32\"\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "model = CLIPModel.from_pretrained(MODEL_ID).to(device)\n",
    "processor = CLIPProcessor.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6784f1db",
   "metadata": {
    "id": "6784f1db"
   },
   "source": [
    "## Setup data model\n",
    "\n",
    "The dataset itself has an image field and an integer label.\n",
    "We'll also need an embedding vector (CLIP produces 512D vectors) field.\n",
    "\n",
    "For this project, field named \"vector\" to the Image class below that is a 512D vector.\n",
    "\n",
    "The image that comes out of the raw dataset is a PIL image. So we'll add some conversion code between PIL and bytes to make it easier for serde."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027bd8e2",
   "metadata": {
    "id": "027bd8e2"
   },
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "from lancedb.pydantic import LanceModel, vector\n",
    "import PIL\n",
    "\n",
    "class Image(LanceModel):\n",
    "    image: bytes\n",
    "    label: int\n",
    "    vector: vector(512)\n",
    "\n",
    "    def to_pil(self):\n",
    "        return PIL.Image.open(io.BytesIO(self.image))\n",
    "\n",
    "    @classmethod\n",
    "    def pil_to_bytes(cls, img) -> bytes:\n",
    "        buf = io.BytesIO()\n",
    "        img.save(buf, format=\"PNG\")\n",
    "        return buf.getvalue()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44277d19",
   "metadata": {
    "id": "44277d19"
   },
   "source": [
    "## Image processing function\n",
    "\n",
    "Next we will implement a function to process batches of data from the dataset.\n",
    "We will be using the `zh-plus/tiny-imagenet` dataset from huggingface datasets.\n",
    "This dataset has an `image` and a `label` column.\n",
    "\n",
    "We extract image embeddings from\n",
    "the image using the CLIP model.\n",
    "\n",
    "Here we'll use the `get_image_features` method on the model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c040600",
   "metadata": {
    "id": "5c040600"
   },
   "outputs": [],
   "source": [
    "def process_image(batch: dict) -> dict:\n",
    "    image = processor(text=None, images=batch[\"image\"], return_tensors=\"pt\")[\n",
    "        \"pixel_values\"\n",
    "    ].to(device)\n",
    "    img_emb = model.get_image_features(image)\n",
    "    batch[\"vector\"] = img_emb.cpu()\n",
    "    batch[\"image_bytes\"] = [Image.pil_to_bytes(img) for img in batch[\"image\"]]\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839ed9f2",
   "metadata": {
    "id": "839ed9f2"
   },
   "source": [
    "## Table creation\n",
    "We create a LanceDB table called `image_search` to store the image, label, and vector.\n",
    "\n",
    "Let's call `lancedb.connect` then\n",
    "`db.create_table`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64a6057",
   "metadata": {
    "id": "e64a6057"
   },
   "outputs": [],
   "source": [
    "import lancedb\n",
    "\n",
    "db = lancedb.connect(\"~/.lancedb\")\n",
    "TABLE_NAME = \"image_search\"\n",
    "db.drop_table(TABLE_NAME, ignore_missing=True)\n",
    "table = db.create_table(TABLE_NAME, schema=Image.to_arrow_schema())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c0cc14",
   "metadata": {
    "id": "e0c0cc14"
   },
   "source": [
    "## Adding data\n",
    "\n",
    "Now we're ready to process the images and generate embeddings.\n",
    "We write a function called `datagen` that calls `process_image` on each image in the validation set (10K images) and return a list of Image instances.\n",
    "\n",
    "\n",
    "1. We may find it faster to use the [dataset.map](https://huggingface.co/docs/datasets/process#map) function.\n",
    "2. We'll want to store the `image_bytes` field that is returned by `process_image`.\n",
    "3. We'll call `load_dataset` and retrieve the `valid` split\n",
    "4. We then map `process_image` and construct Image instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f40c825",
   "metadata": {
    "id": "4f40c825"
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def datagen() -> list[Image]:\n",
    "    dataset = load_dataset(\"zh-plus/tiny-imagenet\")['valid']\n",
    "    return [Image(image=b[\"image_bytes\"],\n",
    "                 label=b[\"label\"],\n",
    "                 vector=b[\"vector\"])\n",
    "           for b in dataset.map(process_image, batched=True, batch_size=64)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f2fc633",
   "metadata": {
    "id": "2f2fc633"
   },
   "source": [
    "Now we call the function we just wrote and add the generated instances to the LanceDB table. The following process can take up to 60 minutes to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8a5c2e",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 457,
     "referenced_widgets": [
      "984bbceeb1144dfe9b30748c00da5d5c",
      "0d4425af76ca463c8a133ef86764f179",
      "dc9ba5ca2a524795847f41f5da582e82",
      "3e0af6f94dea499c9555253392e0a7ff",
      "a83c12d04e6f45598104d72d631166eb",
      "0a4d6d70863c4032b78066f172f5049a",
      "7f46d1d2c02849868074367f7e4ba1a1",
      "aac2cc6c1e6a49c99f95227f9137ad8e",
      "2047f93dd17b43558298b05462945c52",
      "6816d94aa9a74ded95dbf9fbaaa5b4fb",
      "e85368d985da4bf6be57d823dea751ab",
      "b7290f5502bb45169fac837b71b90af1",
      "b7dcccefb9e84712af53345cdcf11080",
      "222ebbac98ca42d193502aca6d2ea705",
      "28e455daa2424748af2c6bd3fffd93c6",
      "db5c923834114891a6ec4a3958a37c3a",
      "4404631cdd2b464599791375b2c8a3ad",
      "cf19ea9340ae4e39806f61982f368af2",
      "a52fa69b3ff54ed48aae7f8dee9e0769",
      "c59482c9fc944199a8ae454cd7a7f288",
      "50c6e1ebf30045b0bf100c2579455d57",
      "edda73774eac4ab791b7c69f6979702e",
      "965e55c6884c4652b280125002c8cca2",
      "960f12671c664f32b99f7e4f48fc8d76",
      "2427fe3587024bc69a8841ebda275c49",
      "85d8ca95e48b4eb1967d8b0100a26893",
      "eb5107ca3c0f44aaa489fb1467f32294",
      "4f9a6bd231d64c1d81a95c77dd966425",
      "748c56c6ffc04f7191b8a8bd3e6c57b1",
      "8c99f6a3209e4615af7436eb0ca57fe7",
      "2936114cdc8b4b17893001c925751571",
      "92c7d96a5d8d47749982d26b30152e0b",
      "1eb4f92dcb32418593174311a7e7abeb",
      "066ad074787849ce90e74f013508fd02",
      "cc44a6fb7c254724913617e1fa320c41",
      "2234b222c44c4de39572a8b65684aef7",
      "43707c491e984213b9304f57ce79fb9d",
      "c72154d07b3f44a78506152db1442b7b",
      "89985e09c75c44299f3c1afac5bdb243",
      "640617f021064faea7669b0ce4ede270",
      "83a4a3fb8b6b4a4c969834ad8540ef05",
      "5d75cb025bf543aebe1d473ce44d0220",
      "43aff4ce3718401aba01cecdd58ba90e",
      "5855f6bbc70e4c349f54573e393b758a",
      "31fe072f16b547288269e21d28e0f1f7",
      "7cf0540e8dd54a0090627457b9b0413c",
      "368d567b16c049b6a3e7bccb09dc6e22",
      "5253909c227047b89d3269865d59f5f1",
      "cdb8267eb7b64157a61595bbc79f1c36",
      "8638c7acb46446dca6ed9390531d84d8",
      "20ab9f3cd7d24c5e90bc4c86d910ca46",
      "1cb40b17579c47db82cc47461910c35e",
      "3d7c5f91a89a46468edb68fa38dcbf74",
      "acb349a9eb214e598bd5de6ec0494fe9",
      "c9417f83aebc4d67842d4c3bbeb0f966",
      "364ba474bf1f458fb573ff951b0c61f3",
      "0351a5adc2c241bfb548f3b02aa12e49",
      "0542e8043cb44de6be54f261b8988c67",
      "c684db23c8f345a282736ba03ad5304f",
      "98dd8f9e26354d7baeb926f20f4ebaf7",
      "7c290d412d3f48259e8d0980209a5926",
      "fd00f3924ba849e199e203c652289528",
      "ead4c3e445f8458ab09f5e6eedba8755",
      "d53c8c3ff7584e579a14d5782776f30d",
      "b0cc4ade1bc64a02976e0d6b7bc59353",
      "5217f11d5c494f58a203d1ec0bbe6bfa",
      "cbb0f67f8059462bac21ac6360e2554e",
      "dc2bf810936b4195adaccab20b86300e",
      "2a6da211e73b4867a75ae3c7194872e2",
      "171dcb3753384b11b8431d30a807a40b",
      "91a5a072abf244738606d5099c1ecf41",
      "bd139c2184264d24a698edda10f13301",
      "fea263b0070f47b388f40c6222cec10a",
      "521f3f35197042b99f3127732b6a53a3",
      "82500760d28b470a92560a6619610cd3",
      "7c5621e19dc04547b04fa40200e180af",
      "166f9f31c2474afc9de43686c79a990e"
     ]
    },
    "executionInfo": {
     "elapsed": 2272530,
     "status": "ok",
     "timestamp": 1753994342160,
     "user": {
      "displayName": "Srishti Jain",
      "userId": "17735186882349502119"
     },
     "user_tz": 240
    },
    "id": "4b8a5c2e",
    "outputId": "21c6074b-bd88-4f73-9626-7279e962a5f5",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984bbceeb1144dfe9b30748c00da5d5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7290f5502bb45169fac837b71b90af1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "dataset_infos.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "965e55c6884c4652b280125002c8cca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-1359597a978bc4fa.parquet:   0%|          | 0.00/146M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "066ad074787849ce90e74f013508fd02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)-00000-of-00001-70d52db3c749a935.parquet:   0%|          | 0.00/14.6M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31fe072f16b547288269e21d28e0f1f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/100000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "364ba474bf1f458fb573ff951b0c61f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating valid split:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/dill/_dill.py:414: PicklingWarning: Cannot locate reference to <class '__main__.Image'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/usr/local/lib/python3.11/dist-packages/dill/_dill.py:414: PicklingWarning: Cannot pickle <class '__main__.Image'>: __main__.Image has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/usr/local/lib/python3.11/dist-packages/dill/_dill.py:414: PicklingWarning: Cannot locate reference to <class '__main__.ColabKernelApp'>.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "/usr/local/lib/python3.11/dist-packages/dill/_dill.py:414: PicklingWarning: Cannot pickle <class '__main__.ColabKernelApp'>: __main__.ColabKernelApp has recursive self-references that trigger a RecursionError.\n",
      "  StockPickler.save(self, obj, save_persistent_id)\n",
      "Parameter 'function'=<function process_image at 0x7faaf258a660> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n",
      "WARNING:datasets.fingerprint:Parameter 'function'=<function process_image at 0x7faaf258a660> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbb0f67f8059462bac21ac6360e2554e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "AddResult(version=2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = datagen()\n",
    "table.add(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb70b75",
   "metadata": {
    "id": "bfb70b75"
   },
   "source": [
    "## Encoding user queries\n",
    "\n",
    "We have image embeddings, but how do we generate the embeddings for the user query?\n",
    "Furthermore, how can we possibly have the same features between the image embeddings\n",
    "and text embeddings. This is where the power of CLIP comes in.\n",
    "\n",
    "We write a function to turn user query text into an embedding\n",
    "in the same latent space as the images.\n",
    "\n",
    "**NOTE**\n",
    "We refer to the [CLIPModel documention](https://huggingface.co/docs/transformers/model_doc/clip#transformers.CLIPModel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460a8aba",
   "metadata": {
    "id": "460a8aba"
   },
   "outputs": [],
   "source": [
    "from transformers import CLIPTokenizerFast\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "tokenizer = CLIPTokenizerFast.from_pretrained(MODEL_ID)\n",
    "\n",
    "def embed_func(query):\n",
    "    inputs = tokenizer([query], padding=True, return_tensors=\"pt\")\n",
    "    text_features = model.get_text_features(**inputs)\n",
    "    return text_features.detach().numpy()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9487085e",
   "metadata": {
    "id": "9487085e"
   },
   "source": [
    "## Core search function\n",
    "\n",
    "Now we write the core search function `find_images`, that takes a text query as input, and returns a list of PIL images that's most similar to the query.\n",
    "\n",
    "\n",
    "First, we need to call `embed_func` on the query to generate the text embedding.\n",
    "Then, we'll search through the LanceDB to get images most similar to the query.\n",
    "And we'll convert the resrults into Image instances.\n",
    "Finally, we'll call the `Image.to_pil` method to convert the bytes into PIL images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c712bcd",
   "metadata": {
    "id": "4c712bcd"
   },
   "outputs": [],
   "source": [
    "def find_images(query):\n",
    "    emb = embed_func(query)\n",
    "    rs = table.search(emb).limit(9).to_pydantic(Image)\n",
    "    return [m.to_pil() for m in rs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ad4f0b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "executionInfo": {
     "elapsed": 188,
     "status": "ok",
     "timestamp": 1753994509461,
     "user": {
      "displayName": "Srishti Jain",
      "userId": "17735186882349502119"
     },
     "user_tz": 240
    },
    "id": "f8ad4f0b",
    "outputId": "8aa16348-20d5-4787-9927-87a365a83dc6"
   },
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/2wBDAAgGBgcGBQgHBwcJCQgKDBQNDAsLDBkSEw8UHRofHh0aHBwgJC4nICIsIxwcKDcpLDAxNDQ0Hyc5PTgyPC4zNDL/2wBDAQkJCQwLDBgNDRgyIRwhMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjIyMjL/wAARCABAAEADASIAAhEBAxEB/8QAHwAAAQUBAQEBAQEAAAAAAAAAAAECAwQFBgcICQoL/8QAtRAAAgEDAwIEAwUFBAQAAAF9AQIDAAQRBRIhMUEGE1FhByJxFDKBkaEII0KxwRVS0fAkM2JyggkKFhcYGRolJicoKSo0NTY3ODk6Q0RFRkdISUpTVFVWV1hZWmNkZWZnaGlqc3R1dnd4eXqDhIWGh4iJipKTlJWWl5iZmqKjpKWmp6ipqrKztLW2t7i5usLDxMXGx8jJytLT1NXW19jZ2uHi4+Tl5ufo6erx8vP09fb3+Pn6/8QAHwEAAwEBAQEBAQEBAQAAAAAAAAECAwQFBgcICQoL/8QAtREAAgECBAQDBAcFBAQAAQJ3AAECAxEEBSExBhJBUQdhcRMiMoEIFEKRobHBCSMzUvAVYnLRChYkNOEl8RcYGRomJygpKjU2Nzg5OkNERUZHSElKU1RVVldYWVpjZGVmZ2hpanN0dXZ3eHl6goOEhYaHiImKkpOUlZaXmJmaoqOkpaanqKmqsrO0tba3uLm6wsPExcbHyMnK0tPU1dbX2Nna4uPk5ebn6Onq8vP09fb3+Pn6/9oADAMBAAIRAxEAPwDjkBBqyg9qc/llx5abVAA69T605Qc4rm5meM7MUAUpQN0q7HY3TrlYmPsRg017WaM5khdPcijnIcWkU/KXuKYUI+7Vor7U1hT5iLsqOAWLEdaZKrModhx0BPerEgGKgk5PCHHvRcuLL1tarLFKP41wQPbv/SrmnRxxEu2N3bParEFiImDrnNUkgleTaOuefasb3HTmmzprOdD3BrUhljJ5VSveuU+W0QBJC0mfmHanJfyDI5IPap5UzfmsbWqaFb3EZmtQI5OuwdG/CuQkXYxVsgg4IIrpdLuREC5VVDHjjAH19Kr6lbwXd00qBAxHz7DwT604trRmNZK3Mc8EU9x9M1GAobJIxWo+nYzhiM1D9gZD91SPU1pdGHMjpRGrAYxVae1bnyk2sxyWOMU/IhxuyaYdQUnCg59MViTC8diobGVSRtz+NK8BghaR1CgDvyTV2GSV1AKH3J4zUt1ayXNo0aqoY4wSeOtFzRTl1Mi0T7QSWTIUdia1o4UVQFHuansrFbSBUOC3Vj6mrJWLIywGfwobIm3JlE24PJUgVA0ShsE1qGMSL8rhl/PNUpIVWQKT8zZIoTI5RuozrGGRdvTkisEXgjnJHPqBWpeo5c/KMHnINZBt2I+7+Iojax0KJr2uoRyADvWvbyI65zyetceI3jwRkGtSwvNrhWP40OPYHE6bGRjC9OlQvbK8qsTjb1469/6VNbyRyxhgQcjFKjI12bcqd4G72I/yazuLlK8sJY5DMMHPpUa7JnwCd68EEfpzzWm8LLUDIcDgEjoaLi5T/9k=\n",
      "image/png": "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\n",
      "text/plain": [
       "<PIL.PngImagePlugin.PngImageFile image mode=RGB size=64x64>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_images(\"fish\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_U4wxvQRZRjQ",
   "metadata": {
    "id": "_U4wxvQRZRjQ"
   },
   "outputs": [],
   "source": [
    "import nbformat\n",
    "\n",
    "path = \"cleaned_notebook.ipynb\"\n",
    "\n",
    "# Get the current notebook contents\n",
    "from google.colab import _message\n",
    "nb = _message.blocking_request('get_ipynb')['ipynb']\n",
    "\n",
    "# Remove 'widgets' metadata if present\n",
    "if 'widgets' in nb['metadata']:\n",
    "    del nb['metadata']['widgets']\n",
    "\n",
    "# Save the cleaned notebook\n",
    "with open(path, \"w\") as f:\n",
    "    nbformat.write(nbformat.from_dict(nb), f)\n",
    "\n",
    "print(f\"Cleaned notebook saved as {path}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
